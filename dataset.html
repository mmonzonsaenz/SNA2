<!DOCTYPE html>
<html lang="en">
<nav style="background-color: #2c3e50; padding: 10px;">
  <a href="/SNA2/index.html" style="color: white; margin-right: 20px;">Home</a>
  <a href="/SNA2/code.html" style="color: white; margin-right: 20px;">View Code</a>
  <a href="/SNA2/dataset.html" style="color: white; margin-right: 20px;">Dataset Generation</a>
  <a href="/SNA2/code.html#downloads" style="color: white;">Downloads</a>
</nav>
<head>
  <meta charset="UTF-8">
  <title>SNA Project â€“ Dataset Generation</title>
  <style>
  nav {
    background-color: #2c3e50;
    padding: 10px;
  }

  nav a {
    color: white;
    margin-right: 20px;
    text-decoration: none;
    font-family: Arial, sans-serif;
    font-weight: bold;
  }

  nav a:hover {
    text-decoration: underline;
  }

body {
  font-family: monospace;
  background-color: white;
  color: #000;
  padding: 40px;
}
    
pre {
  background-color: #f4f4f4; /* or white */
  color: #000;
  padding: 20px;
  border-radius: 8px;
  overflow-x: auto;
  white-space: pre-wrap;
}

  h1, h2 {
    color: #61dafb;
  }

  a {
    color: #61dafb;
    text-decoration: none;
  }

  a:hover {
    text-decoration: underline;
  }

  .container {
    margin-top: 40px;
    padding: 20px;
    background-color: #2d2d2d;
    border-radius: 8px;
  }
  </style>
</head>
<body>
  <h1>Dataset Generation</h1>


  <p>We manually selected different videos from the five chosen parties:</p>
  <pre>
Part 1: We selected different videos from the five chosen parties manually.
Shtefanov :
1. https://www.youtube.com/watch?v=6xIeLJGcpfU&t=1s
2. https://www.youtube.com/watch?v=6uicsdZDw-Y
3. https://www.youtube.com/watch?v=0Q5BY7N6Vv8
4. https://www.youtube.com/watch?v=rX5apWMrf4c
5. https://www.youtube.com/watch?v=41NI4Y6frAQ

Maxim Katz 
1. https://www.youtube.com/watch?v=70bt2dBV1zI
2. https://www.youtube.com/watch?v=0lDM2JVq8Y8
3. https://www.youtube.com/watch?v=qqCYMqoUa5Q&t=10s 
4. https://www.youtube.com/watch?v=4b6068q7Jtk
5. https://www.youtube.com/watch?v=H7KTb_IfJFU
6. https://www.youtube.com/watch?v=6WW_4D3ARVM

FBK (Volkov + Pevchikh + Alburov)
1. https://www.youtube.com/watch?v=WpOLafcjT4Q
2. https://www.youtube.com/watch?v=xQoUvHcXqhw
3. https://www.youtube.com/watch?v=E-NJs8-29yw
4. https://www.youtube.com/watch?v=-_wMvLpOnPQ
5. https://www.youtube.com/watch?v=GioQp_j74eY
6. https://www.youtube.com/watch?v=xxtCsa0j6eM
7. https://www.youtube.com/watch?v=JKIb_qB_dXk
8. https://www.youtube.com/watch?v=hjVe7WztrdY&t=26s
9. https://www.youtube.com/watch?v=C2N83Rx-3kU

Khodorkovsky
1. https://www.youtube.com/watch?v=xVah87LKS04
2. https://www.youtube.com/watch?v=oCSmIZPoxGM
3. https://www.youtube.com/watch?v=p57ikQ0l97Y
4. https://www.youtube.com/watch?v=Qw1fsunO11Q

Part 2: We used Google Colab to extract video titles, video IDs, channels, tags, view counts, like counts, and comment counts into one CSV file.

Step 1: Generate the Google API for further usage

Step 2: Install Required Libraries  
!pip install google-api-python-client pandas

Step 3: Import Libraries  
from googleapiclient.discovery import build  
import pandas as pd

Step 4: Set Up API Key and Video IDs  
API_KEY = 'AIzaSyAUUwuf_vofeMpwo4qcmWHn_a_iWafPE1E'  
VIDEO_IDS = ['6xIeLJGcpfU','6uicsdZDw-Y','0Q5BY7N6Vv8','rX5apWMrf4c',
'41NI4Y6frAQ','70bt2dBV1zI','0lDM2JVq8Y8','qqCYMqoUa5Q','4b6068q7Jtk',
'H7KTb_IfJFU','6WW_4D3ARVM','WpOLafcjT4Q','xQoUvHcXqhw',
'E-NJs8-29yw','_wMvLpOnPQ','GioQp_j74eY','xxtCsa0j6eM','JKIb_qB_dXk',
'hjVe7WztrdY','C2N83Rx-3kU','xVah87LKS04','oCSmIZPoxGM','p57ikQ0l97Y',
'Qw1fsunO11Q']

Step 5: Initialize the YouTube API Client  
youtube = build('youtube', 'v3', developerKey=API_KEY)

Step 6: Define the get_video_metadata Function  
def get_video_metadata(video_ids):  
    video_data = []  
    for video_id in video_ids:  
        request = youtube.videos().list(part="snippet", id=video_id)  
        response = request.execute()  

        for item in response.get("items", []):  
            video_id = item["id"]  
            title = item["snippet"]["title"]  
            channel = item["snippet"]["channelTitle"]  
            tags = item["snippet"].get("tags", [])  
            tags_str = ", ".join(tags)  
            view_count = item["statistics"].get("viewCount", 0)  
            like_count = item["statistics"].get("likeCount", 0)  
            comment_count = item["statistics"].get("commentCount", 0)  

            video_data.append([video_id, title, channel, tags_str, view_count, like_count, comment_count])  
    return video_data  

Step 7: Fetch Metadata  
metadata = get_video_metadata(VIDEO_IDS)

Step 8: Convert to DataFrame  
df = pd.DataFrame(metadata, columns=["Video ID", "Title", "Channel", "Tags", "Views", "Likes", "Comments"])

Step 9: Save DataFrame as a CSV File  
df.to_csv("youtube_video_metadata.csv", index=False, encoding="utf-8")

Step 10: Confirmation Message  
print("Metadata has been saved to youtube_video_metadata.csv!")

Step 11: Download the CSV File  
from google.colab import files  
files.download('youtube_video_metadata.csv')

Part 3: We used Google Colab to extract all the comments from the videos including Video IDs, Author, Comment, Likes, and Publish time.

Step 1: Generate the Google API for further usage

Step 2: Install Required Libraries  
!pip install google-api-python-client pandas

Step 3: Import Libraries  
from googleapiclient.discovery import build  
import pandas as pd

Step 4: Set Up API Key and Video IDs  
(API_KEY same as above)

Step 5: Initialize the YouTube API Client  
youtube = build('youtube', 'v3', developerKey=API_KEY)

Step 6: Define get_comments Function  
def get_comments(video_id):  
    comments = []  
    next_page_token = None  
    while True:  
        request = youtube.commentThreads().list(  
            part="snippet",  
            videoId=video_id,  
            pageToken=next_page_token,  
            maxResults=100,  
            textFormat="plainText"  
        )  
        response = request.execute()  

        for item in response.get("items", []):  
            comment = item["snippet"]["topLevelComment"]["snippet"]["textDisplay"]  
            author = item["snippet"]["topLevelComment"]["snippet"]["authorDisplayName"]  
            like_count = item["snippet"]["topLevelComment"]["snippet"]["likeCount"]  
            published_at = item["snippet"]["topLevelComment"]["snippet"]["publishedAt"]  
            comments.append([video_id, author, comment, like_count, published_at])  

        next_page_token = response.get("nextPageToken")  
        if not next_page_token:  
            break  
    return comments  

Step 7: Fetch Comments for All Video IDs  
all_comments = []  
for video_id in VIDEO_IDS:  
    print(f"Fetching comments for video ID: {video_id}")  
    try:  
        comments = get_comments(video_id)  
        all_comments.extend(comments)  
    except Exception as e:  
        print(f"Error fetching comments for video ID {video_id}: {e}")  

Step 8: Convert Comments to a DataFrame  
df = pd.DataFrame(all_comments, columns=["Video ID", "Author", "Comment", "Likes", "Published At"])

Step 9: Save the DataFrame to a CSV File  
df.to_csv("youtube_video_metadata.csv", index=False, encoding="utf-8")

Step 10: Confirmation Message  
print("Metadata has been saved to youtube_video_metadata.csv!")

Step 11: Download the CSV File  
from google.colab import files  
files.download('youtube_comments.csv')

  </pre>
</body>
</html>
